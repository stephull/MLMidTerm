{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4DCPQWsDb6k"
      },
      "source": [
        "<b>\n",
        "  Stephen Hullender\n",
        "</b>\n",
        "<br/>\n",
        "<span>\n",
        "  CIS 4526 - Foundations in Machine Learning\n",
        "  (Fall 2022)\n",
        "</span>\n",
        "<br/>\n",
        "<span>\n",
        "  Midterm Assignment: Paraphrase Identification\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i54O-M8zDcee"
      },
      "source": [
        "<br/>\n",
        "<h3>Getting Started</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY1fk6n93e6y",
        "outputId": "e7f1219a-f73e-4812-e359-13ed8decf8f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "np.set_printoptions(threshold=100)\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "#pd.set_option('display.max_rows', sys.maxsize)\n",
        "\n",
        "# SKlearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, multilabel_confusion_matrix, mean_absolute_error\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# scipy\n",
        "from scipy import linalg\n",
        "from scipy.stats import wasserstein_distance\n",
        "import scipy.spatial as sp\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import WordPunctTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# other \n",
        "from google.colab import drive, files\n",
        "import pickle\n",
        "import time\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import calendar "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQzy4x1l5tBo",
        "outputId": "7aa01c1d-12ab-4dab-aaa0-b1d1ee354c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Text files have been converted to Pandas DataFrames...\n"
          ]
        }
      ],
      "source": [
        "# define source folder and text file names w/ labels needed for each table\n",
        "SRC = \"gdrive/My Drive/MLMidterm\"\n",
        "SRC_TRAIN = f\"{SRC}/train_with_label.txt\"\n",
        "SRC_DEV = f\"{SRC}/dev_with_label.txt\"\n",
        "SRC_TEST = f\"{SRC}/test_without_label.txt\"\n",
        "LABELS = [\"instance_id\", \"sentence_1\", \"sentence_2\", \"gold_label\"]\n",
        "\n",
        "# conversion of txt to DataFrame\n",
        "def load_txt_to_pd():\n",
        "  global train, dev, test\n",
        "  train = pd.read_csv(SRC_TRAIN, delimiter=\"\\t\", names=LABELS, on_bad_lines='skip', encoding='utf-8')\n",
        "  dev = pd.read_csv(SRC_DEV, delimiter=\"\\t\", names=LABELS, on_bad_lines='skip', encoding='utf-8')\n",
        "  test = pd.read_csv(SRC_TEST, delimiter=\"\\t\", names=LABELS[:-1], on_bad_lines='skip', encoding='utf-8')\n",
        "  # encoding: https://dev.to/_aadidev/3-ways-to-handle-non-utf-8-characters-in-pandas-242\n",
        "  print(\"Text files have been converted to Pandas DataFrames...\")\n",
        "\n",
        "# connect to Google Drive to fetch files\n",
        "try:  \n",
        "  load_txt_to_pd()\n",
        "except:\n",
        "  drive.mount('/content/gdrive')\n",
        "  load_txt_to_pd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxn2251hlze5"
      },
      "source": [
        "<h3>Data Cleaning and Preprocessing</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hjp2jEtt6oOB"
      },
      "outputs": [],
      "source": [
        "# take out nan values from all tables\n",
        "train = train.dropna().reset_index(drop=True)\n",
        "dev = dev.dropna().reset_index(drop=True)\n",
        "test = test.dropna().reset_index(drop=True)\n",
        "\n",
        "# band-aids\n",
        "test = test.drop([69])\n",
        "test.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "J5gpD4fd9DoF"
      },
      "outputs": [],
      "source": [
        "# along with LABELS, make a TABLES array\n",
        "TABLES = [train, dev, test]\n",
        "\n",
        "gold_train = train['gold_label'].tolist()\n",
        "gold_dev = dev['gold_label'].tolist()\n",
        "\n",
        "# labels missing (showing text instead) on the following indexes:\n",
        "training_label_band_aids = [\n",
        "    319, 818, 904, 1007, 1076, 1089, 1132, 1258, 1408, 1476, 1546, 1664, 1735, 1789, 1857,\n",
        "    2012, 2088, 2356, 2784, 2999, 3087, 3299, 3368, 3451, 3630, 3813\n",
        "]\n",
        "#corresponding_train_labels = [\n",
        "#    1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1\n",
        "#]\n",
        "corresponding_train_labels = [0] * 26\n",
        "for i in range(len(training_label_band_aids)):\n",
        "  index = training_label_band_aids[i]\n",
        "  gold_train[index] = str(corresponding_train_labels[i])\n",
        "\n",
        "# another band aid\n",
        "dev_label_band_aids = [77, 415, 602]\n",
        "#corresponding_dev_labels = [1, 0, 1]\n",
        "corresponding_dev_labels = [0] * 3\n",
        "for i in range(len(dev_label_band_aids)):\n",
        "  index = dev_label_band_aids[i]\n",
        "  gold_dev[index] = str(corresponding_dev_labels[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SaSucOejyF2o"
      },
      "outputs": [],
      "source": [
        "contractions = { \n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"I'd\": \"I would\",\n",
        "  \"I'd've\": \"I would have\",\n",
        "  \"I'll\": \"I will\",\n",
        "  \"I'll've\": \"I will have\",\n",
        "  \"I'm\": \"I am\",\n",
        "  \"I've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it would\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"that'd\": \"that had\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there would\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we would\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you would\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you will\",\n",
        "  \"you'll've\": \"you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cHlSosxM07ne"
      },
      "outputs": [],
      "source": [
        "def regex(s):\n",
        "  s = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', s, flags=re.MULTILINE)\n",
        "  s = re.sub(r'\\<a href', ' ', s)\n",
        "  s = re.sub(r'&amp;', '', s) \n",
        "  s = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', s)\n",
        "  s = re.sub(r'<br />', ' ', s)\n",
        "  s = re.sub(r'\\'', ' ', s)\n",
        "  return s;\n",
        "\n",
        "def stops(s):\n",
        "  s = word_tokenize(s)\n",
        "  stops = set(stopwords.words('english'))\n",
        "  words = []\n",
        "  for word in s:\n",
        "    if not word in stops:\n",
        "      words.append(word)\n",
        "  return \" \".join(words)\n",
        "\n",
        "def clean_text(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = sentence.split()\n",
        "  text = []\n",
        "  for word in sentence:\n",
        "    text.append(contractions[word] if word in contractions else word)\n",
        "  sentence = \" \".join(text)\n",
        "  sentence = regex(sentence)\n",
        "  sentence = stops(sentence)\n",
        "  sentence = WordPunctTokenizer().tokenize(sentence)\n",
        "  return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NG_zLCiof7Qe"
      },
      "outputs": [],
      "source": [
        "# use the functions above to parse all words into separate elements in a list\n",
        "# and take out punctuation and stopwords\n",
        "for table in TABLES:\n",
        "  # clean first column\n",
        "  clean1 = list(map(clean_text, table['sentence_1']))\n",
        "  table['new_sentence_1'] = clean1\n",
        "  # clean second column\n",
        "  clean2 = list(map(clean_text, table['sentence_2']))\n",
        "  table['new_sentence_2'] = clean2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OdpMPqbtG_C1"
      },
      "outputs": [],
      "source": [
        "# lemmatize text, last step in cleaning words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for table in TABLES:\n",
        "  # lemmatize first column\n",
        "  lemma1 = list(map(\n",
        "    lambda word: list(map(lemmatizer.lemmatize, word)),\n",
        "    table['new_sentence_1']\n",
        "  ))\n",
        "  table['new_sentence_1'] = lemma1\n",
        "  # lemmatize second column\n",
        "  lemma2 = list(map(\n",
        "    lambda word: list(map(lemmatizer.lemmatize, word)),\n",
        "    table['new_sentence_2']\n",
        "  ))\n",
        "  table['new_sentence_2'] = lemma2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ae4oUFyCXUos"
      },
      "outputs": [],
      "source": [
        "# drop original text columns & reorder columns\n",
        "train.drop(['sentence_1', 'sentence_2'], axis=1, inplace=True)\n",
        "dev.drop(['sentence_1', 'sentence_2'], axis=1, inplace=True)\n",
        "test.drop(['sentence_1', 'sentence_2'], axis=1, inplace=True)\n",
        "new_cols = ['new_sentence_1', 'new_sentence_2', 'gold_label']\n",
        "train = train.reindex(columns=new_cols)\n",
        "dev = dev.reindex(columns=new_cols)\n",
        "test = test.reindex(columns=new_cols[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjjHKb0wXuUm"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkLdYoaYZebb"
      },
      "outputs": [],
      "source": [
        "dev.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhQHH_4QaL9_"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5honRC2Xta87"
      },
      "outputs": [],
      "source": [
        "n1 = train['new_sentence_1'] ; n2 = train['new_sentence_2']\n",
        "nd1 = dev['new_sentence_1'] ; nd2 = dev['new_sentence_2']\n",
        "nx1 = test['new_sentence_1'] ; nx2 = test['new_sentence_2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AGei8fK7DXrQ"
      },
      "outputs": [],
      "source": [
        "# fetch length of each row of word\n",
        "len_n1 = [len(n1[i]) for i in range(len(n1))]\n",
        "len_n2 = [len(n2[i]) for i in range(len(n2))]\n",
        "len_nd1 = [len(nd1[i]) for i in range(len(nd1))]\n",
        "len_nd2 = [len(nd2[i]) for i in range(len(nd2))]\n",
        "len_nx1 = [len(nx1[i]) for i in range(len(nx1))]\n",
        "len_nx2 = [len(nx2[i]) for i in range(len(nx2))]\n",
        "\n",
        "# calculate differences and average for each pair of columns\n",
        "lenavg_train = [((len_n1[i] + len_n2[i]) / 2) for i in range(len(len_n1))]\n",
        "lenavg_dev = [((len_nd1[i] + len_nd2[i]) / 2) for i in range(len(len_nd1))]\n",
        "lenavg_test = [((len_nx1[i] + len_nx2[i]) / 2) for i in range(len(len_nx1))]\n",
        "lendiff_train = [abs(len_n1[i] - len_n2[i]) / 2 for i in range(len(len_n1))]\n",
        "lendiff_dev = [abs(len_nd1[i] - len_nd2[i]) for i in range(len(len_nd1))]\n",
        "lendiff_test = [abs(len_nx1[i] - len_nx2[i]) for i in range(len(len_nx1))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cyOMhaXe-iZH"
      },
      "outputs": [],
      "source": [
        "# separate TfidfVectorizer's for each array/Series of text data\n",
        "tfidf_n1 = TfidfVectorizer(use_idf=True, smooth_idf=True, preprocessor=\" \".join, stop_words='english', lowercase=False)\n",
        "tfidf_n2 = TfidfVectorizer(use_idf=True, smooth_idf=True, preprocessor=\" \".join, stop_words='english', lowercase=False)\n",
        "tfidf_nd1 = TfidfVectorizer(use_idf=True, smooth_idf=True, preprocessor=\" \".join, stop_words='english', lowercase=False)\n",
        "tfidf_nd2 = TfidfVectorizer(use_idf=True, smooth_idf=True, preprocessor=\" \".join, stop_words='english', lowercase=False)\n",
        "tfidf_nx1 = TfidfVectorizer(use_idf=True, smooth_idf=True, preprocessor=\" \".join, stop_words='english', lowercase=False)\n",
        "tfidf_nx2 = TfidfVectorizer(use_idf=True, smooth_idf=True, preprocessor=\" \".join, stop_words='english', lowercase=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vZ-8MK3tlNe"
      },
      "outputs": [],
      "source": [
        "# matrices of numbers from TfidfVectorizer\n",
        "arr_n1 = tfidf_n1.fit_transform(n1).toarray()\n",
        "arr_n2 = tfidf_n2.fit_transform(n2).toarray()\n",
        "arr_nd1 = tfidf_nd1.fit_transform(nd1).toarray()\n",
        "arr_nd2 = tfidf_nd2.fit_transform(nd2).toarray()\n",
        "arr_nx1 = tfidf_nx1.fit_transform(nx1).toarray()\n",
        "arr_nx2 = tfidf_nx2.fit_transform(nx2).toarray()\n",
        "\n",
        "print(len(arr_n1)); print(len(arr_nd1)); print(len(arr_nx1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IIlDvRrRteQA"
      },
      "outputs": [],
      "source": [
        "# add x number of arrays of same length to accomodate for missing words\n",
        "# get words that are available in one set but not another\n",
        "train_1_not_2 = list(set(tfidf_n1.get_feature_names_out()).difference(tfidf_n2.get_feature_names_out()))\n",
        "train_2_not_1 = list(set(tfidf_n2.get_feature_names_out()).difference(tfidf_n1.get_feature_names_out()))\n",
        "dev_1_not_2 = list(set(tfidf_nd1.get_feature_names_out()).difference(tfidf_nd2.get_feature_names_out()))\n",
        "dev_2_not_1 = list(set(tfidf_nd2.get_feature_names_out()).difference(tfidf_nd1.get_feature_names_out()))\n",
        "test_1_not_2 = list(set(tfidf_nx1.get_feature_names_out()).difference(tfidf_nx2.get_feature_names_out()))\n",
        "test_2_not_1 = list(set(tfidf_nx2.get_feature_names_out()).difference(tfidf_nx1.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "poahUeE7PwVt"
      },
      "outputs": [],
      "source": [
        "# take each existing matrix (2 cells above) and add empty rows\n",
        "\n",
        "# change feature names to fit all words\n",
        "new_arr_n1 = []\n",
        "for i in range(len(arr_n1)):\n",
        "    temp = list(arr_n1[i])\n",
        "    temp.extend([0] * len(train_2_not_1))\n",
        "    new_arr_n1.append(temp)\n",
        "arr_n1 = np.array(new_arr_n1)\n",
        "\n",
        "new_arr_n2 = []\n",
        "for i in range(len(arr_n2)):\n",
        "    temp = list(arr_n2[i])\n",
        "    temp.extend([0] * len(train_1_not_2))\n",
        "    new_arr_n2.append(temp)\n",
        "arr_n2 = np.array(new_arr_n2)\n",
        "\n",
        "new_arr_nx1 = []\n",
        "for i in range(len(arr_nx1)):\n",
        "    temp = list(arr_nx1[i])\n",
        "    temp.extend([0] * len(test_2_not_1))\n",
        "    new_arr_nx1.append(temp)\n",
        "arr_nx1 = np.array(new_arr_nx1)\n",
        "\n",
        "new_arr_nx2 = []\n",
        "for i in range(len(arr_nx2)):\n",
        "    temp = list(arr_nx2[i])\n",
        "    temp.extend([0] * len(test_1_not_2))\n",
        "    new_arr_nx2.append(temp)\n",
        "arr_nx2 = np.array(new_arr_nx2)\n",
        "\n",
        "new_arr_nd1 = []\n",
        "for i in range(len(arr_nd1)):\n",
        "    temp = list(arr_nd1[i])\n",
        "    temp.extend([0] * len(dev_2_not_1))\n",
        "    new_arr_nd1.append(temp)\n",
        "arr_nd1 = np.array(new_arr_nd1)\n",
        "\n",
        "new_arr_nd2 = []\n",
        "for i in range(len(arr_nd2)):\n",
        "    temp = list(arr_nd2[i])\n",
        "    temp.extend([0] * len(dev_1_not_2))\n",
        "    new_arr_nd2.append(temp)\n",
        "arr_nd2 = np.array(new_arr_nd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "O81YbAc0PwK-"
      },
      "outputs": [],
      "source": [
        "# conjoin features\n",
        "features_train = list(tfidf_n1.get_feature_names_out())\n",
        "extra_train = list(train_2_not_1)\n",
        "features_train.extend(extra_train)\n",
        "\n",
        "features_dev = list(tfidf_nd1.get_feature_names_out())\n",
        "extra_dev = list(dev_2_not_1)\n",
        "features_dev.extend(extra_dev)\n",
        "\n",
        "features_test = list(tfidf_nx1.get_feature_names_out())\n",
        "extra_test = list(test_2_not_1)\n",
        "features_test.extend(extra_test)\n",
        "\n",
        "# all lengths are to be equal to set.union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "bQR7hto3e27n"
      },
      "outputs": [],
      "source": [
        "# process separate DataFrames for each table (TFIDF)\n",
        "def process_tfidf(table, values):\n",
        "  return pd.DataFrame(\n",
        "      data=table.T, index=values, columns=([f'tfidf_{i}' for i in range(len(table))])\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b40n4OlKT3N"
      },
      "outputs": [],
      "source": [
        "# for training column 1\n",
        "df_n1 = process_tfidf(arr_n1, features_train)\n",
        "# for training column 2\n",
        "df_n2 = process_tfidf(arr_n2, features_train)\n",
        "# for dev column 1\n",
        "df_nd1 = process_tfidf(arr_nd1, features_dev)\n",
        "# for dev column 2\n",
        "df_nd2 = process_tfidf(arr_nd2, features_dev)\n",
        "# for test column 1\n",
        "df_nx1 = process_tfidf(arr_nx1, features_test)\n",
        "# for test column 2\n",
        "df_nx2 = process_tfidf(arr_nx2, features_test)\n",
        "# example\n",
        "df_n1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3cnLeeAIm7hR"
      },
      "outputs": [],
      "source": [
        "# test cosine similarity of two matrices using scipy\n",
        "\n",
        "def calculate_cosine(n1, n2):\n",
        "  cos = []\n",
        "  for i in range(len(n1)):\n",
        "    temp1, temp2 = n1[i], n2[i]\n",
        "    c = sp.distance.cosine(temp1, temp2)\n",
        "    cos.append(c)\n",
        "  return cos\n",
        "\n",
        "cosine_results_train = calculate_cosine(arr_n1, arr_n2)\n",
        "cosine_results_dev = calculate_cosine(arr_nd1, arr_nd2)\n",
        "cosine_results_test = calculate_cosine(arr_nx1, arr_nx2)\n",
        "\n",
        "# HOW IT WORKS: \n",
        "# sp.distance.cosine(num[], num[]) -> int\n",
        "# input1 <- first num[]\n",
        "# input2 <- second num[]\n",
        "# return (\n",
        "#   1 - (\n",
        "#     input1 (dot) input2 \n",
        "#     /                  \n",
        "#     np.sqrt( add x^2 for x in input1 ) x np.sqrt( add x^2 for x in input2 )\n",
        "#   )\n",
        "# )\n",
        "# MORE: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "njSqX2fBxikj"
      },
      "outputs": [],
      "source": [
        "# save text_similarities in pickle file since they take too long\n",
        "pickle_ts1 = f'{SRC}/text_similarities_train.pickle'\n",
        "pickle_ts2 = f'{SRC}/text_similarities_dev.pickle'\n",
        "pickle_ts3 = f'{SRC}/text_similarities_test.pickle'\n",
        "pickle_cs1 = f'{SRC}/compare_train.pickle'\n",
        "pickle_cs2 = f'{SRC}/compare_dev.pickle'\n",
        "pickle_cs3 = f'{SRC}/compare_test.pickle'\n",
        "\n",
        "if not os.path.exists(pickle_ts1):\n",
        "  open(pickle_ts1, 'wb').close()\n",
        "if not os.path.exists(pickle_ts2):\n",
        "  open(pickle_ts2, 'wb').close()\n",
        "if not os.path.exists(pickle_ts3):\n",
        "  open(pickle_ts3, 'wb').close()\n",
        "if not os.path.exists(pickle_cs1):\n",
        "  open(pickle_cs1, 'wb').close()\n",
        "if not os.path.exists(pickle_cs2):\n",
        "  open(pickle_cs2, 'wb').close()\n",
        "if not os.path.exists(pickle_cs3):\n",
        "  open(pickle_cs3, 'wb').close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KBSh__5ZoYGV"
      },
      "outputs": [],
      "source": [
        "# try gensim\n",
        "# source: https://betterprogramming.pub/introduction-to-gensim-calculating-text-similarity-9e8b55de342d\n",
        "from gensim import corpora, models, similarities\n",
        "\n",
        "text_similarities_train = []\n",
        "if not os.path.getsize(pickle_ts1) > 0:\n",
        "  for IND in range(len(arr_n1)):\n",
        "    #start = 0 if IND < len(arr_n1)-500 else len(arr_n1)-500 \n",
        "    end = IND+500\n",
        "    if (end > len(arr_n1)):\n",
        "      end = len(arr_n1)\n",
        "    text_a = n1[0:end]     # require array for dictionary\n",
        "    text_b = n2[IND]\n",
        "    dictionary = corpora.Dictionary(text_a)\n",
        "    corpus = [dictionary.doc2bow(t) for t in text_a]  # put each individual entry in doc2bow\n",
        "    tfidf = models.TfidfModel(corpus)\n",
        "    kw_vector = dictionary.doc2bow(text_b)\n",
        "    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.token2id))\n",
        "    sim = index[tfidf[kw_vector]]\n",
        "    text_similarities_train.append(sim[IND])\n",
        "  with open(pickle_ts1, 'wb') as writefile:\n",
        "    pickle.dump(text_similarities_train, writefile)\n",
        "    writefile.close()\n",
        "else:\n",
        "  readfile = open(pickle_ts1, 'rb')\n",
        "  text_similarities_train = pickle.load(readfile)\n",
        "  readfile.close()\n",
        "\n",
        "\n",
        "text_similarities_dev = []\n",
        "if not os.path.getsize(pickle_ts2) > 0:\n",
        "  for IND in range(len(arr_nd1)):\n",
        "    end = IND+500\n",
        "    if (end > len(arr_nd1)):\n",
        "      end = len(arr_nd1)\n",
        "    text_a = nd1[0:end]    \n",
        "    text_b = nd2[IND]\n",
        "    dictionary = corpora.Dictionary(text_a)\n",
        "    corpus = [dictionary.doc2bow(t) for t in text_a] \n",
        "    tfidf = models.TfidfModel(corpus)\n",
        "    kw_vector = dictionary.doc2bow(text_b)\n",
        "    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.token2id))\n",
        "    sim = index[tfidf[kw_vector]]\n",
        "    text_similarities_dev.append(sim[IND])\n",
        "  with open(pickle_ts2, 'wb') as writefile:\n",
        "    pickle.dump(text_similarities_train, writefile)\n",
        "    writefile.close()\n",
        "else:\n",
        "  readfile = open(pickle_ts2, 'rb')\n",
        "  text_similarities_dev = pickle.load(readfile)\n",
        "  readfile.close()\n",
        "\n",
        "\n",
        "text_similarities_test = []\n",
        "if not os.path.getsize(pickle_ts3) > 0:\n",
        "  for IND in range(len(arr_nx1)):\n",
        "    end = IND+500\n",
        "    if (end > len(arr_nx1)):\n",
        "      end = len(arr_nx1)\n",
        "    text_a = nx1[0:end] \n",
        "    text_b = nx2[IND]\n",
        "    dictionary = corpora.Dictionary(text_a)\n",
        "    corpus = [dictionary.doc2bow(t) for t in text_a] \n",
        "    tfidf = models.TfidfModel(corpus)\n",
        "    kw_vector = dictionary.doc2bow(text_b)\n",
        "    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=len(dictionary.token2id))\n",
        "    sim = index[tfidf[kw_vector]]\n",
        "    text_similarities_test.append(sim[IND])\n",
        "  with open(pickle_ts3, 'wb') as writefile:\n",
        "    pickle.dump(text_similarities_train, writefile)\n",
        "    writefile.close()\n",
        "else:\n",
        "  readfile = open(pickle_ts3, 'rb')\n",
        "  text_similarities_test = pickle.load(readfile)\n",
        "  readfile.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbK2UVis9fhF"
      },
      "source": [
        "<h3>\n",
        "  Using Algorithms\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-JxjHsIk8Y7r"
      },
      "outputs": [],
      "source": [
        "def print_results(a, p):\n",
        "  lbl = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "  ind = ['weighted', 'micro', 'macro']\n",
        "  acc = []\n",
        "  pre = []\n",
        "  rec = []\n",
        "  ff = []\n",
        "\n",
        "  acc_score = accuracy_score(a, p)\n",
        "  for i in range(len(ind)):\n",
        "    acc.append(acc_score)\n",
        "    pre.append(\n",
        "        precision_score(a, p, average=ind[i])\n",
        "    )\n",
        "    rec.append(\n",
        "        recall_score(a, p, average=ind[i])\n",
        "    )\n",
        "    ff.append(\n",
        "        f1_score(a, p, average=ind[i])\n",
        "    )\n",
        "\n",
        "  dta = [acc, pre, rec, ff]\n",
        "  df = pd.DataFrame(dta, columns=ind, index=lbl)\n",
        "\n",
        "  ts = str(calendar.timegm(time.gmtime()))\n",
        "  filename = f'{SRC}/results-{ts}.txt'\n",
        "\n",
        "  results = pd.DataFrame()\n",
        "  with open(filename, 'a') as f:\n",
        "    dfAsString = df.to_string(header=True, index=True)\n",
        "    f.write(dfAsString + \"\\n\")\n",
        "    f.close()\n",
        "\n",
        "  print(' ::: weighted, micro, macro :::')\n",
        "  print('ACCURACY: ', acc_score)\n",
        "  print('PRECISION: ', pre)\n",
        "  print('RECALL: ', rec)\n",
        "  print('F1-SCORE: ', ff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aFve0A8W6-w1"
      },
      "outputs": [],
      "source": [
        "# before performing algorithms, we want to know whether the following influences the decision of accurately predicting the gold label:\n",
        "# - cosine similarity\n",
        "# - similarity based on sparse matrix\n",
        "# - average length in pair of text\n",
        "# - difference in length of pair of text\n",
        "# - & for train/dev comparisons, add gold_label\n",
        "\n",
        "CL = ['cosine', 'similarity', 'length_average', 'length_difference', 'label']\n",
        "\n",
        "compare_data_train = []\n",
        "if not os.path.getsize(pickle_cs1) > 0:\n",
        "  for i in range(len(gold_train)):\n",
        "    temp = (cosine_results_train[i], text_similarities_train[i], lenavg_train[i], lendiff_train[i], gold_train[i])\n",
        "    compare_data_train.append(temp)\n",
        "  with open(pickle_cs1, 'wb') as writefile:\n",
        "    pickle.dump(compare_data_train, writefile)\n",
        "    writefile.close()\n",
        "else:\n",
        "  readfile = open(pickle_cs1, 'rb')\n",
        "  compare_data_train = pickle.load(readfile)\n",
        "  readfile.close()\n",
        "compare_train = pd.DataFrame(data=compare_data_train, columns=CL)\n",
        "\n",
        "\n",
        "compare_data_dev = []\n",
        "if not os.path.getsize(pickle_cs2) > 0:\n",
        "  for i in range(len(gold_dev)):\n",
        "    temp = (cosine_results_dev[i], text_similarities_dev[i], lenavg_dev[i], lendiff_dev[i], gold_dev[i])\n",
        "    compare_data_dev.append(temp)\n",
        "  with open(pickle_cs2, 'wb') as writefile:\n",
        "    pickle.dump(compare_data_dev, writefile)\n",
        "    writefile.close()\n",
        "else:\n",
        "  readfile = open(pickle_cs2, 'rb')\n",
        "  compare_data_dev = pickle.load(readfile)\n",
        "  readfile.close()\n",
        "compare_dev = pd.DataFrame(data=compare_data_dev, columns=CL)\n",
        "\n",
        "\n",
        "compare_data_test = []\n",
        "if not os.path.getsize(pickle_cs3) > 0:\n",
        "  for i in range(len(cosine_results_test)):\n",
        "    temp = (cosine_results_test[i], text_similarities_test[i], lenavg_test[i], lendiff_test[i])\n",
        "    compare_data_test.append(temp)\n",
        "  with open(pickle_cs3, 'wb') as writefile:\n",
        "    pickle.dump(compare_data_test, writefile)\n",
        "    writefile.close()\n",
        "else:\n",
        "  readfile = open(pickle_cs3, 'rb')\n",
        "  compare_data_test = pickle.load(readfile)\n",
        "  readfile.close()\n",
        "compare_test = pd.DataFrame(data=compare_data_test, columns=CL[0:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bgA8u4V0m2Un"
      },
      "outputs": [],
      "source": [
        "# logistic regression (condition: estimate label based on cosine AND gensim similarlity)\n",
        "x = compare_train.drop('label', axis=1)\n",
        "y = compare_train['label']\n",
        "\n",
        "lr_1 = LogisticRegression()\n",
        "\n",
        "# 1 -> training data for training    -> matrix\n",
        "# 2 -> data for prediction (testing) -> matrix\n",
        "# 3 -> label column for training                          -> series\n",
        "# 4 -> data for comparing prediction results (testing)    -> series\n",
        "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=0.3, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV37uxuW9kN-"
      },
      "outputs": [],
      "source": [
        "# start with training and predict via training data (split)\n",
        "lr_1.fit(x_train, y_train)\n",
        "preds_1 = lr_1.predict(x_test)\n",
        "\n",
        "print_results(y_test, preds_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zE5r09p9kSA"
      },
      "outputs": [],
      "source": [
        "# next, try with training and predict via dev data\n",
        "a = compare_dev.drop('label', axis=1)\n",
        "b = compare_dev['label']\n",
        "\n",
        "# x, y, a, b\n",
        "lr_1.fit(x.values, y.values)\n",
        "preds_2 = lr_1.predict(a.values)\n",
        "\n",
        "print_results(b.values, preds_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6RmLTf79kWJ"
      },
      "outputs": [],
      "source": [
        "# use same logistic regression for predicting gold labels\n",
        "final_preds = lr_1.predict(compare_data_test)\n",
        "print(final_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwuZ0o4husal"
      },
      "source": [
        "<h3>\n",
        "  Sources\n",
        "</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFic5mrWuvXf"
      },
      "source": [
        "<span>\n",
        "  External Links\n",
        "</span>\n",
        "<br/>\n",
        "<ul>\n",
        "<li><a href=\"https://github.com/stephull/MLMidTerm\" target='_blank'>GitHub (Forked Repo)</a></li>\n",
        "</ul>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "559124b0723559227e5d54d54845b037ad1c01998faeb822d7c1fc61c0b623fd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
